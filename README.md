# transformer-Attention-is-All-You-Need

Google的《Attention is all you need》学习记录与总结。
源代码参考了https://github.com/Kyubyong/transformer。

论文地址：https://arxiv.org/pdf/1706.03762.pdf

这篇论文的主要思想是放弃Rnn，全程只有attention来解决问题。
这篇论文的介绍已经很多了，我说点我自己的理解，并且，我想探索一下这篇论文的灵感是怎么来的，把论文中的核心思想和现实生活中的事情联系起来。

RNN，包括LSTM等，在处理一句话时，都是一个字一个字的处理数据。我们每个人刚认字时，也都是一个字一个字的读文章的，但现在，我们高中毕业，大学毕业后，恐怕不会
还是一个字一个字的读文章了，不是一目十行，也得一目半行吧，而且大家都知道，在读文章时，很多时候打乱一句话中各个字的顺序，并不会影响人们的理解，
所以，我深刻的怀疑这篇论文的灵感就是来源于人们从小到大读文章的习惯。

